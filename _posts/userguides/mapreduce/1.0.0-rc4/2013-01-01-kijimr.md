---
title: Kiji MapReduce
layout: post
categories: [userguides, mapreduce, 1.0.0-rc4]
tags: [mapreduce-ug]
version: 1.0.0-rc4
order : 10
description: Kiji MapReduce user guide.
---

# Overview

Kiji MapReduce is the second major component in the Kiji ecosystem for Apache Hadoop and HBase after KijiSchema.

Kiji MapReduce includes APIs to build MapReduce jobs that read and write data stored in Kiji tables, bringing MapReduce-based analytic techniques to a broad base of KijiSchema users, to support applications such as machine learning and analysis.

KijiMR is organized around three core MapReduce job types: _Bulk Importers_, _Producers_ and _Gatherers_. These may read from external data sources through _KeyValueStores_.

 * _Bulk Importers_ make it easy to efficiently load data into Kiji tables from a variety of formats, such as JSON or CSV files in HDFS clusters.
 * _Producers_ are entity-centric operations that use a entity's existing data to generate new information and store it back in the entity's row. One typical use-case for producers is to generate new recommendations for a user based on the user's history.
 * _Gatherers_ provide flexible MapReduce computations that scan over Kiji table rows and output key-value pairs. By using different outputs and reducers, Gatherers can export data to external data stores such as files in HDFS clusters in a variety of formats (such as text or avro) or even into HFiles to efficiently load data into other Kiji Tables.

Finally, Kiji MapReduce allows any of these jobs to combine their data with external _KeyValueStores_ as a way to join data sets store in HDFS, in Kiji, etc.

Unlike Kiji Schema, where the classes most relevant to application developers were usually concrete, these core job types exist in Kiji MapReduce as abstract classes (such as `KijiProducer`). It is typically up to the application developer to subclass the appropriate class in their application and implement their application's analysis logic in a few methods (such as `KijiProducer`'s `produce()` and `getDataRequest()`). They can then point the job at the appropriate Kiji table using either the `kiji` command-line tools or programmatically using one of the framework's JobBuilders (such as `KijiProduceJobBuilder`) that make launching these jobs easy.  Jobs within Kiji can record relevant job related metadata to provide a historical view.

To provide a starting point and some useful precanned solutions, the complementary Kiji MapReduce Library provides a growing repository of sample implementations that can be used directly or as example code. Both Kiji MapReduce and the Kiji MapReduce Library are included in the Kiji Bento Box distribution.

In the sections of this guide that follow, the core job types will be explained in greater detail, including motivation, example code snippets, and where appropriate, a description of the Kiji MapReduce Library's reference implementations. This guide also contains an in-depth description of how to use Kiji MapReduce's _KeyValueStores_ to expose data stored in different places and formats (such as HDFS or KijiTables) through a consistent interface to your MapReduce jobs. For power users, we've included instructions on how to use the `KijiMapReduceJobBuilder` class and `kiji mapreduce` command to build and launch arbitrary MapReduce jobs using Kiji mappers and reducers to perform flexible data transformation.

Finally this guide contains a description of the command-line tools included with Kiji MapReduce and facilities that make it easier to test Kiji MapReduce application code.

# Kiji MapReduce framework

## Bulk Importing

### Motivation

Before we can analyze any data in a Kiji table, we have to get that data into the Kiji table.

Using Kiji Schema alone, one can load data into a Kiji table from a single machine using a simple program with a while loop. For very small jobs, the speed of one machine may be fast enough, but for bigger jobs, a distributed approach is needed. With a little elaboration, one could distribute the simple program's work in the form of a MapReduce job whose mappers write to the Kiji table in parallel. However, writing directly to Kiji Schema's underlying HBase from a MapReduce job can introduce heavy load to a cluster making things sluggish or even unstable.

To more efficiently import data into Kiji tables, Kiji MapReduce includes _BulkImporters_. A bulk importer is a MapReduce job that processes its input into files that can be bulk loaded directly into Kiji. The format of the input and how it translates into Kiji table entity IDs and columns are details particular to each concrete subclass of `KijiBulkImporter`.

Kiji MapReduce Lib contains a set of sample parsers and bulk importers that enable users to import data for many common data formats (including CSV, JSON, and the Apache Common Log Format) into a Kiji table using an Avro description of how the data maps into the table's layout. See the _Provided Library Classes_ section below for more information.

### Classes Overview

Kiji bulk importers rely on two classes:
* _KijiBulkImporter_ - the base class for concrete bulk importers.  Subclasses must implement the `produce()` method for converting the raw data into Kiji puts, as well as the `setup()` and `teardown()` methods if they wish to perform any special configuration at task startup or shutdown respectively.
* _KijiBulkImportJobBuilder_ - a MapReduce job builder that allows the creation of bulk import jobs using concrete bulk importers.

Bulk-importers inherit from `KijiBulkImporter` and must implement their bulk-importing logic in the `produce()` method.
Optionally, Bulk-importers may use the `setup()` and `cleanup()` methods to initialize and finalize resources that can be shared across input records.
These methods will be called once by each task, `setup()` before processing input records and `cleanup()` after the task is done processing input.

### Using the API

If one of the precanned bulk importers listed below is insufficient for the data that is to be imported,
the KijiBulkImporter can be extended to support parsing and importing the desired data.
The `produce()` method needs to be implemented to handle the extraction of data from the import text.

Once you have the requisite bulk-importer, importing the data can simply follow these steps:

*   Define any necessary configuration for the bulk importer.
    For example, the bulk importers in Kiji MapReduce Library that use the DescribedInputTextBulkImporter all require a table import descriptor to map between source data and destination data.

*   Once the proper configuration file has been written, the data can be bulk imported into Kiji via commands like:

### Example

{% highlight java %}
/**
 * Example of a Bulk-importer.
 *
 * Reads a text file formatted as "rowKey;integerValue",
 * and emits the integer value in the specified row into the column:
 *     "imported_family:int_value_column"
 *
 * Each line from the input text file is converted into an input key/value pair
 * by the Hadoop text input format, where:
 *   <li> the key is the offset of the line in the input file,
 *        as a LongWritable named 'filePos';
 *   <li> the value is the line content, as a Text object named 'value'.
 */
public class BulkImporterExample extends KijiBulkImporter<LongWritable, Text> {
  public static enum Counters {
    INVALID_INPUT_LINE,
    INVALID_INTEGER,
  }

  /** {@inheritDoc} */
  @Override
  public void produce(LongWritable filePos, Text value, KijiTableContext context)
      throws IOException {
    // Process one line from the input file (filePos is not used in this example):
    final String line = value.toString();

    // Line is expected to be formatted as "rowKey;integerValue":
    final String[] split = line.split(";");
    if (split.length != 2) {
      // Record the invalid line and move on:
      context.incrementCounter(Counters.INVALID_INPUT_LINE);
      return;
    }
    final String rowKey = split[0];
    try {
      final int integerValue = Integer.parseInt(split[1]);

      // Write a cell in row named 'rowKey', at column 'imported:int_value':
      final EntityId eid = context.getEntityId(rowKey);
      context.put(eid, "imported_family", "int_value_column", integerValue);

    } catch (NumberFormatException nfe) {
      // Record the invalid integer and move on:
      context.incrementCounter(Counters.INVALID_INTEGER);
      return;
    }
  }
}
{% endhighlight %}

This bulk-importer may be run from the console with a command like the following. This command assumes that our input lives as a text file in HDFS and that our output table, named `number_table` here, has already been installed with the correct layout on the default Kiji instance.

{% highlight bash %}
kiji bulk-import \
    --importer=my.application.package.BulkImporterExample \
    --input="format=text file=hdfs://cluster/path/to/text-input-file \
    --output="format=kiji table=kiji://.env/default/table nsplits=1" \
{% endhighlight %}

This will launch a mapreduce job to bulk load the table.

See the commandline section of this userguide for a more comprehensive list of options on the command-line interface.

### Provided Library Classes

Within the `org.kiji.mapreduce.lib.bulkimport` package of the Kiji MapReduce Library, there is a variety of useful parsers for building your own bulk importer:
* `CSVParser` - parses delimited CSV(Comma Separated Value) data into the component fields.  This parser also handles TSV(Tab Separated Value) data.
* `CommonLogParser` - parses Common Log Format data(used by Apache web server) into the relevant fields for each request in the log.

There are several associated bulk importers that parse data into rows:
* `CSVBulkImporter` - takes in CSV files and produces a row for each line in the file.
* `CommonLogBulkImporter` - takes in an Apache web server log and produces a row for each client request.
* `JSONBulkImporter` - takes an a text file with a JSON object on each line and produces a row for each object.

All of these bulk importers extend `DescribedInputTextBulkImporter` which contains helper functions and can be configured via a `KijiTableImportDescriptor` object. `KijiTableImportDescriptor` is an Avro-based specification that translates from the inferred schemas inside of the input files to the existing Kiji table layout.

See the javadoc for these classes for instructions and examples on using them.

## Gatherers

### Motivation

A Kiji Gatherer scans over the rows of a Kiji table using the MapReduce framework and output key-value pairs. Gatherers are a flexible job type and can be used to extract or aggregate information into a variety of formats based on the output specification and reducer used.
Common tasks for gatherers include calculating sums across an entire table, extracting features to train a model, and pivoting information from one table into another. You should use a gatherer when you need to pull data out of a Kiji table into another format or to feed it into a Reducer.

### Classes Overview

There are three classes for an application that wants to use gatherers:

All gatherers extend the abstract class
`org.kiji.mapreduce.gather.KijiGatherer` and override its abstract methods as
described below. Clients should be familiar with the
`org.kiji.mapreduce.gather.GatherContext` class, which is used to output the
Gatherer's key-value pairs. Finally, while gather jobs can be launched from the
commandline with `kiji gather`,
`org.kiji.mapreduce.gather.KijiGatherJobBuilder` can be used to construct a
MapReduce job that runs a given gather with configured input, output, and
reducer. The job can then be launched programmatically.

## Using the API

All gatherers must extend the parameterized class `KijiGatherer` with the types
of the key and value of their output. Concrete gatherers must implement these methods:

 * `Class<?> getOutputKeyClass()` and `Class<?> getOutputValueClass()`. These methods should return the
classes of the output keys and values emitted by the gatherer.
 * `KijiDataRequest getDataRequest()`. This methods specifies the columns retrieved while scanning rows from the input table. It should construct and return a `KijiDataRequest`.
 * `void gather(KijiRowData input, GathererContext<K, V> context)`. This methods contains the gatherer's logic that translates its input into key-value pairs. It will be called once per row processed by the gatherer task. `input` will contain the columns from the row as requested by the `KijiDataRequest` returned by `getDataRequest()`. The gatherer should use its `context` parameter to emit key-value pairs as detailed below.

While processing a row, a gatherer may access data from external stores through `KeyValueStore`s by implementing `Map<String, KeyValueStore<?, ?>> getRequiredStores()`. This method should construct and return a map specifying all the `KeyValueStore`s that the gatherer wants to access. The `KeyValueStore`s may then later be accessed from the `gather()` method through the `GathererContext`. For more details, you may check the [KeyValue Stores](#ref.kvstore) section in this guide.

Optionally, a gatherer may implement `setup()` and `cleanup()` to initialize and
finalize resources that can be reused during the gather task. These methods
will be called once by each task, `setup()` before processing input rows and
`cleanup()` after the task is done processing. If you wish to use a
`KeyValueStore`, it should be opened once with `context.getStore(storeName)` in
`setup()`, saved in a member variable, and closed in `cleanup()`.

The class of a gatherer's output key and value may have restrictions
depending on the output format used with this gatherer. For example, if used with a sequence file output format, classes must either be Avro types
or implement the `org.apache.hadoop.io.Writable` interface. See the Command-line Tools section of this guide for more about output format options.

As mentioned above, a gatherer's `gather` method has a `org.kiji.mapreduce.gather.GatherContext` argument. This class has a number of methods which are relevant to a gatherer author:

 * `void write(K key, V value)`. Emits a key-value pair. The key-value pair will be later processed by 
the reduce step of this MapReduce job. The type of `key` and `value` should match the
parameter types of the gatherer's class.
 * `void incrementCounter(Enum<?> counter)`. Increments a mapreduce job counter. This can be useful for calculating aggregate counts about the full MapReduce job (for example, you can use a counter to calculate the total number of input rows containing malformed data). This method is common to all KijiContexts.
 
### Example

{% highlight java %}
/**
 * Example of a gatherer class that extracts the size of households per zip-code.
 *
 * Processes entities from an input table that contains households with:
 *   <li> a column 'info:zip_code' with the zip code of a household;
 *   <li> a column 'members:*' with the household's members names;
 * and emits one key/value pair per household to a sequence file on HDFS, where:
 *   <li> the key is the zip code of the household, as an IntWritable,
 *   <li> the value is the number of persons in the household, as an IntWritable.
 */
public class FamilySizeGatherer extends KijiGatherer<IntWritable, IntWritable> {
  public static enum Counters {
    MISSING_ZIP_CODE,
  }

  /** {@inheritDoc} */
  @Override
  public Class<?> getOutputKeyClass() {
    // Zip code encoded as an IntWritable:
    return IntWritable.class;
  }

  /** {@inheritDoc} */
  @Override
  public Class<?> getOutputValueClass() {
    // Family size (number of persons) encoded as an IntWritable:
    return IntWritable.class;
  }

  /** {@inheritDoc} */
  @Override
  public KijiDataRequest getDataRequest() {
    // Fetch columns 'info:zip_code' and map-family 'members':
    return KijiDataRequest.builder()
        .addColumns(ColumnsDef.create()
            .add("info", "zip_code")
            .addFamily("members"))
        .build();
  }

  /** {@inheritDoc} */
  @Override
  public void gather(KijiRowData input, GathererContext<IntWritable, IntWritable> context)
      throws IOException {
    // Extract the required data from the input row:
    final Integer zipCode = input.getMostRecentValue("info", "zip_code");
    if (zipCode == null) {
      // Zip code is missing from the input row, report the bad input and move on:
      context.incrementCounter(Counters.MISSING_ZIP_CODE);
      return;
    }

    // Since we only care about the size of the family,
    // rather than the data stored for each member,
    // we only extract the qualifiers for the "members" family.
    final Set<String> members = input.getQualifiers("members");

    // Some computation: for this simple example,
    // we compute the family size from the set of its members:
    final int familySize = members.size();

    // Emit a pair (zip code, family size) to the configured output through the context:
    context.write(new IntWritable(zipCode), new IntWritable(familySize));
  }
}
{% endhighlight %}

The gatherer described above may be used on the command-line as follows.
It expects a Kiji input table whose rows represent households,
with a column `'info:zip_code'` that contains the Zip code of each household
and with a column family `'members'` that lists the household members.

{% highlight bash %}
kiji gather \
    --gatherer=my.application.package.FamilySizePerZipCodeGatherer \
    --input="format=kiji table=kiji://.env/default/household_table_name" \
    --output="format=seq file=hdfs://cluster/path/to/sequence-file nsplits=1"
{% endhighlight %}

With this command, the gather job will write its output key-value pairs to sequence files
in the directory `hdfs://cluster/path/to/sequence-file/`. Each entry in the sequence file
will correspond to one household and will contain the household Zip code as a key and
the number of household members as a value.

Each gather task will produce a single sequence file `hdfs://cluster/path/to/sequence-file/part-m-<gather task #>`.
The number of gatherer tasks is currently set to be the number of regions in the input table: each gather task
processes one region from the input table.
Therefore, the `nsplits` parameter of the job output specification is not used in this context when no reducer is specified.

### Provided Library Classes (Optional)


## Producers

### Motivation

A KijiProducer executes a function over a subset of the columns in a table row and produces output to be injected back into a column of that row.
Producers can be run in the context of a MapReduce over entire Kiji tables, or on-demand over a single row at a time.
Common tasks for producers include parsing, profiling, recommending, predicting, and classifying.
For example, you might run a LocationIPProducer to compute and store the location of each user into a new column,
or a PersonalizationProfileProducer to compute a personalization profile.

Whereas Gatherers generally run over the rows of a Kiji table to generate key value pairs for the purposes of conversion, analysis, or filling in information in a different table, Producers can only write back information directly into the row they're presently processing. Producers are the most appropriate tool when you want to update a row with information calculated from the existing contents of that row and KeyValueStores.

### Classes Overview

There are three important classes for an application that wants to use producers:

All producers must extend the abstract class `org.kiji.mapreduce.produce.KijiProducer` and override its abstract methods as described below. Clients should be familiar with the `org.kiji.mapreduce.producer.ProducerContext` class, which is the interface producers use to output data. Finally, while produce jobs can be launched with `kiji produce` from the commandline, `org.kiji.mapreduce.produce.KijiProduceJobBuilder` can be used to construct a  mapreduce job that uses a given producer. The job can then be launched programmatically.

### Using the API

Each producer must extend `KijiProducer` and must implement the following three methods:

 * `KijiDataRequest getDataRequest()`. This method specifies the columns retrieved while scanning from the input table. It should construct and return a `KijiDataRequest`.
 * `String getOutputColumn()`. This method specifies the fully qualified column or the column family 'being produced'. It should return a string of the form "family:qualifier" or "family". Family-only output columns are only valid for map-type families (see the KijiSchema user guide).
 * `void produce(KijiRowData input, ProducerContext context)`. This method contains the logic to produce the content for the output column for each input row. It will be called once per row processed by the task. `input` contains columns from the row as requested by the `KijiDataRequest` returned from `getDataRequest()`. The `produce()` method can use its `context` argument to output to this column as detailed below.

When producing new content for a row, the producer may combine the input row data with data from external stores through `KeyValueStore`s by implementing `Map<String, KeyValueStore<?, ?>> getRequiredStores()`. This method should construct and return a map specifying all the `KeyValueStore`s that this producer wants to access. The `KeyValueStore`s may then be accessed from the `produce()` method through the `ProducerContext`. For more details, you may check the [KeyValue Stores](#ref.kvstore) section in this guide.

Optionally, a producer may implement `setup()` and `cleanup()` to initialize and finalize resources that can be reused during the produce task.
These methods will be called once by each task, `setup()` before processing input row and `cleanup()` after the task is done processing. If you wish to use a `KeyValueStore`, it should be opened once with `context.getStore(storeName)` in `setup()`, saved in a member variable, and closed in `cleanup()`.

As mentioned above, a Producer's `produce()` method has a `org.kiji.mapreduce.producer.ProducerContext` argument. This class has a number of methods which are important to a producer author:

* `void put(T value)`, `void put(long timestamp, T value)`, `void put(String qualifier, T value)`, and `void put(String qualifier, long timestamp, T value)`. Each of these calls put data into the current row in the column specified by the producer's `getOutputColumn()`. The type of `value` must be compatible with the output column's type as declared in the table layout. The `timestamp` parameter is optional; if ommitted the current time will be used. A `qualifier` argument should only be used if the producer's `getOutputColumn()` specified a map-type family.
* `void incrementCounter(Enum<?> counter)`. Increments a mapreduce job counter. This can be useful for calculating aggregate counts about the full map reduce job (for example, you can use a counter to calculate the total number of input rows containing malformed data). This method is common to all KijiContexts.

### Example

Following is a minimal example of a Kiji producer class. This producer is designed to calculate and save the zodiac signs of people in a user table, and could be part of something like horoscope application. We assume the user table has a column `info:birthday` containing the user's birthdate as a string and a map-type family of strings, `produced`. Our example producer will output the zodiac sign as a string into `produced:zodiac_sign`.

For brevity, the calculation of a zodiac sign from a birthday string has been omitted.

{% highlight java %}
/**
 * Example of a producer class. Calculates zodiac signs.
 */
public static class ZodiacProducer extends KijiProducer {
  public static enum Counters {
    MISSING_BIRTHDAY,
  }

  /** {@inheritDoc} */
  @Override
  public KijiDataRequest getDataRequest() {
    // Fetch all columns in family 'info' from the input table:
    return KijiDataRequest.create("info");
  }

  /** {@inheritDoc} */
  @Override
  public String getOutputColumn() {
    // Configure the producer to emit to a single column 'produced:zodiac_sign':
    return "produced:zodiac_sign";
  }

  /** Compute the zodiac sign from a birthday. */
  private String zodiacSignFromBirthday(String birthday) {
    // Implementation left to the reader
    // …
  }

  /** {@inheritDoc} */
  @Override
  public void produce(KijiRowData row, ProducerContext context) throws IOException {
    // Extract the required data from the input row:
    final CharSequence birthday = input.getMostRecentValue("info", "birthday");
    if (birthday == null) {
      // Input row contains no birthday, report and move on:
      context.incrementCounter(Counters.MISSING_BIRTHDAY);
      return;
    }

    // Some computation:
    final String zodiacSign = zodiacSignFromBirthday(birthday.toString());

    // Emits the generated content to the configured output column:
    context.put(zodiacSign);
  }
}
{% endhighlight %}

Our example producer may be used on the command-line with something like the following. This assumes that our table is named `user_table` installed on the default Kiji instance

{% highlight bash %}
kiji produce \
    --producer=my.application.package.ZodiacProducer \
    --input="format=kiji table=kiji://.env/default/user_table" \
    --output="format=kiji table=kiji://.env/default/usertable nsplits=1"
{% endhighlight %}

Note: the output table of a producer must match the input table. The `nsplits` parameter is ignored for producers.

### Provided Library Classes

The `org.kiji.mapreduce.lib.produce` package of the Kiji MapReduce Library contains a number of producer implementations that might be of use to application developers:

* `AllVersionsSingleInputProducer` and `SingleInputProducer` are convenience classes. Subclasses of these abstract classes only have to implement `String getInputColumn()` instead of constructing an entire `KijiDataRequest` in `getDataRequest()`. The `produce()` method will receive all the versions of that column (if the parent class is `AllVersionsSingleInputProducer`) or the most recent (if the parent class is `SingleInputProducer`).
* `RegexProducer` is an abstract subclass of `SingleInputProducer`. Subclasses must implement `String getInputColumn()` to specify a column and `String getRegex()`, which should be a regular expression that matches the contents of the input column. The regex should have one capturing group. `RegexProducer` contains a `produce()` method which will write that group to the output column. Both input and output should be strings.
* `ConfiguredRegexProducer` is a concrete implementation of `RegexProducer` which uses Configuration keys to specify its input, output, and regular expression. You can use this class to copy substrings from one column to another without writing java code.
* `IdentityProducer` is another concrete producer. It copies data directly from its input column to its output column, both of which may be specified using configuration keys on the commandline. This can be useful for clients who want to copy one column's data to another without writing code.

See the javadoc of these classes for more information about using them.

## Reducers

### Motivation

The key-value pairs generated by Mapper or Gatherer tasks can be aggregated according to their keys by the MapReduce framework. All the values associated to the same key can then be processed by a Reducer that can finally emit new aggregated key-value pairs.

This makes reducers an important component of the KijiMapReduce workflow:
A gatherer can output key-value pairs for each row processed in isolation, but to compute aggregate statistics for the entire table, gatherers must be complemented with appropriate reducers.

### Classes Overview

All Kiji reducers extend the base class `KijiReducer` and must implement three methods:
*  `Class getOutputKeyClass()` and `Class getOutputValueClass()` to specify the classes of the output keys and values it emits;
*  `void reduce(InputKey key, Iterable<InputValue> values, Context context)` to implement the logic to reduce the set of input values for one input key in.
   The type of the input keys and input values are specific to each reducer.
   The `reduce()` method emits reduced (key, value) pairs using the reducer context with `context.write(key, value)`.

Optionally, a reducer may use `setup()` and `cleanup()` to initialize and finalize resources that can be reused during the reduce task.
These methods will be called once by each task, `setup()` before processing input records and `cleanup()` after the task is done processing input.

### Using the API

### Example

Here is a simplistic example of a reducer class that may be chained with the `FamilySizeGatherer` described in the gatherer section:

{% highlight java %}
/**
 * Example of a reducer class that, for every zip code, calculates
 * the number of families per family size.
 *
 * For its input, this reducer expects the output from the FamilySizeGatherer
 * presented in the gatherer section of the user guide.
 * This gatherer emits a key-value pair for every household with:
 *   <li> the household zip code as a key;
 *   <li> the number of members in the household as a value.
 *
 * For its output, this reducer emits key-value pairs where:
 *   <li> keys are strings encoded as "zip-code:family-size";
 *   <li> values are the number of families in the zip code with the size encoded in the key.
 */
public class FamilySizeReducer
    extends KijiReducer<IntWritable, IntWritable, Text, IntWritable>
    implements Configurable {

  /** {@inheritDoc} */
  @Override
  public Class<?> getOutputKeyClass() {
    // Pair (zip code, family size) encoded as a Text formatted "zip-code:family-size":
    return Text.class;
  }

  /** {@inheritDoc} */
  @Override
  public Class<?> getOutputValueClass() {
    // Number of families whose size if the size encoded in the key:
    return IntWritable.class;
  }

  /** {@inheritDoc} */
  @Override
  protected void reduce(IntWritable zipCode, Iterable<IntWritable> familySizes, Context context)
      throws IOException, InterruptedException {
    // Map: family size -> number of families with this size
    final Map<Integer, AtomicInteger> sizeMap = Maps.newHashMap();

    // Count the number of families with :
    for (IntWritable familySize : familySizes) {
      AtomicInteger count = sizeMap.get(familySize.get());
      if (count == null) {
        count = new AtomicInteger(0);
        sizeMap.put(familySize.get(), count);
      }

      // Increment the counter for this family size:
      count.incrementAndGet();
    }

    // Emit the generated family size map for zipCode:
    for (Map.Entry<Integer, AtomicInteger> entry : sizeMap.entrySet()) {
      final int familySize = entry.getKey();
      final int familyCount = entry.getValue().get();
      context.write(
          new Text(String.format("%s:%s", zipCode, familySize)),
          new IntWritable(familyCount));
    }
  }
}
{% endhighlight %}

This reducer may be chained to the FamilySizeGatherer described in the gatherer section of this user guide as follows:

{% highlight bash %}
kiji gather \
    --gatherer=my.application.package.FamilySizeGatherer \
    --reducer=my.application.package.FamilySizeReducer \
    --input="format=kiji table=kiji://.env/default/household_table_name" \
    --output="format=seq file=hdfs://localhost:9000/reducer_output.seq nsplits=3"
{% endhighlight %}

This command-line run a gatherer with the reducer described above.
The output of the job will be written as 3 Hadoop sequence files in HDFS in the directory specified with `hdfs://localhost:9000/reducer_output.seq/`.
Each reducer task will write a sequence file named `hdfs://localhost:9000/reducer_output.seq/part-r-<reducer #>`.
The number of gatherer tasks is currently set to be the number of regions in the input table.
The number of reducer tasks, hence the number of `part` files, is configurable via the `nsplits` job output parameter.


### Provided Library Classes

The `org.kiji.mapreduce.lib.reduce` package of the Kiji MapReduce Library comes with a number of reducers. The most immediately useful ones for an application developer include `org.kiji.mapreduce.lib.reduce.IntSumReducer`, `org.kiji.mapreduce.lib.reduce.LongSumReducer` and `org.kiji.mapreduce.lib.reduce.DoubleSumReducer`. These are KijiReducer versions of reducers that compute the sums of all the values associated with each key.

These reducers are examples of subclasses of the abstract class `org.kiji.mapreduce.lib.reduce.KeyPassThroughReducer`, which is a convenience superclass for KijiReducers that use the same keys as the map phase.

See the KijiMapReduce Library JavaDoc for more information on these classes.

### <a id="ref.table_reducer"> Table reducers </a>

`org.kiji.mapreduce.KijiTableReducer` is a specialization of a `KijiReducer` that writes cell 
into an output Kiji table.
A table reduce must implement the logic to reduce the input values for an input key in
`reduce(InputKey key, Iterable<InputValue> values, KijiTableContext context)`,
using the table reducer `context` to write cells to the configured output table.

Below is an minimal example of a table reducer. This reducer is very similar to the
`FamilySizeReducer` above, but inherits from `KijiTableReducer`, and is designed to output
to hfiles or directly into a Kiji table.

{% highlight java %}
/**
 * Example of a table reducer class that, for every zip code, calculates
 * the number of families per family size.
 *
 * For its input, this reducer expects the output from the FamilySizeGatherer
 * presented in the gatherer section of the user guide.
 * This gatherer emits a key-value pair for every household with:
 *   <li> the household zip code as a key;
 *   <li> the number of members in the household as a value.
 *
 * The output of this reducer is a table whose row keys are zip codes.
 * The reducer writes cells to a map-type family named 'sizes'.
 * Each cell is written to the column 'sizes:<family-size>' and contains
 * the number of families with the size encoded in the column qualifier.
 */
public class FamilySizeTableReducer
    extends KijiTableReducer<IntWritable, IntWritable> {

  /** {@inheritDoc} */
  @Override
  protected void reduce(
      IntWritable zipCode, Iterable<IntWritable> familySizes, KijiTableContext context)
      throws IOException {
    // Map: family size -> number of families with this size
    final Map<Integer, AtomicInteger> sizeMap = Maps.newHashMap();

    // Count the number of families with :
    for (IntWritable familySize : familySizes) {
      AtomicInteger count = sizeMap.get(familySize.get());
      if (count == null) {
        count = new AtomicInteger(0);
        sizeMap.put(familySize.get(), count);
      }

      // Increment the counter for this family size:
      count.incrementAndGet();
    }

    // Emit the generated family size map in the entity whose ID is zipCode:
    final EntityId eid = context.getEntityId(zipCode.toString());

    for (Map.Entry<Integer, AtomicInteger> entry : sizeMap.entrySet()) {
      final Integer familySize = entry.getKey();
      final int familyCount = entry.getValue().get();
      context.put(eid, "sizes", familySize.toString(), familyCount);
    }
  }
}
{% endhighlight %}

This reducer may be chained to the FamilySizeGatherer described in the gatherer section of this user guide as follows:

{% highlight bash %}
kiji gather \
    --gatherer=my.application.package.FamilySizeGatherer \
    --reducer=my.application.package.FamilySizeTableReducer \
    --input="format=kiji table=kiji://.env/default/households_table_name" \
    --output="format=kiji table=kiji://.env/default/family_size_table_name nsplits=3"
{% endhighlight %}

This command-line runs a gatherer chained with the table reducer presented above.
The output of the job is written to the live Kiji table specified with `kiji://.env/default/family_size_table_name`.
The number of gatherer tasks is currenlty set to be the number of regions in the input table.
The number of reducer tasks is controlled by the `nsplits` job output parameter, 3 in this example.

## Transforms

### Motivation

A  Kiji Transformer scans over a file in HDFS and uses the MapReduce framework to aggregate information which can be passed to a Reducer.
This is useful for analyzing data that already lives inside of HDFS, but does not live in Kiji.
This allows you to use the same Mappers and Reducers that are used in other Kiji jobs.

### Classes Overview

Transform jobs are created using the KijiTransformJobBuilder.
Transform jobs require more setup because their data does not exist in Kiji, and so the inputs and outputs need to be specified explicitly.
Inside of the KijiTransformBuilder, you'll find the withMapper() and the withReducer methods to be able to explicitly specify your map and reduce tasks.
To specify the type of the input required for this, the withInput method is called

*   `withInput(MapReduceJobInput jobInput)` - specifies the type of the input for a KijiTransformJobBuilder. Possible examples include:

    * `WholeTextFileMapReduceJobInput` - reads entire text files as records for a map task.

    * `SequenceFileMapReduceJobInput` - reads in sequence files

    * `TextMapReduceJobInput` - reads in text files

    * `AvroKeyValueMapReduceJobInput` - reads in Avro container files of generic records where each entry has a "key" and a "value" field

    * `AvroKeyMapReduceJobInput` - reads in Avro container files

`TODO` Can users write their own input classes? How?


### Using the API


### Example

### Provided Library Classes (Optional)


# Command Line Tools

The KijiMR framework provides command-line tools to submit and monitor MapReduce jobs.

## Overview of available tools

Kiji MapReduce provides the following command-line tools:
*   `kiji bulk-import`: runs a bulk-importer job that imports data from an external data source into a Kiji table.
*   `kiji produce`: runs a producer job.
*   `kiji gather`: runs a gatherer job that processes the rows from a Kiji table and writes files, optionally using a reducer.
*   `kiji bulk-load`: bulk-loads the HFile output of a job into a KijiTable.
*   `kiji mapreduce`: runs an arbitrary Map/Reduce job using Kiji mappers and reducers.
*   `kiji job-history`: retrieve information about jobs previously run through kiji if enabled.

## Using common-flags

Kiji commands bulk-import, produce and gather all recognize the following flags:

*   `--kvstores=/path/to/store-specifications.xml`: specifies the path of XML file describing the key/value stores used by the job.

*   `--lib=/path/to/jars-directory/`: specifies a directory of jar files that contain user code.


*   `--start-row=`: specifies the HBase row to start scanning at (inclusive).
    Example: `--start-row=hex:0088deadbeef`, or `--start-row="utf8:the row key in UTF8"`.

*   `--limit-row=`: specifies the HBase row to stop scanning at (exclusive).
    Example: `--limit-row=hex:0088deadbeef`, or `--limit-row=:utf8:the row key in UTF8"`.


Bulk importers must specify the name of the class providing the bulk-import logic:

*   `--importer=java.package.BulkImporterClassName`: specifies the KijiBulkImporter class to use.

Producers must specify the name of the class providing the producers logic:

*   `--producer=java.package.ProducerClassName`: specifies the KijiProducer class to use.

Gatherer must specify the name of the class providing the gathering logic, and optionally a reducing logic:

*   `--gatherer=java.package.GathererClassName`: specifies the KijiGatherer class to use.

*   `--combiner=java.package.CombinerClassName`: optionally specifies a Combiner class to use.

*   `--reducer=java.package.ReducerClassName`: optionally specifies a Reducer class to use.

## Input/output formats

Jobs inputs and outputs are specified with the following flags:

*   `--input=...`: specifies the input of the job.

    The job input specification is formatted as `--input="format= ..."`.
    Kiji recognizes the following job input formats:

    * `avro`: job input is an Avro container file, each input record is a pair (Avro key, NullWritable).

    * `avrokv`: job input is an Avro container file for key/value generic records.

    * `htable`: job input is an HTable, each input record is a pair (HBase row key, HBase Result).
      The address of the HBase cluster is pull from the local job configuration (ie. from the HBase configuration available on the classpath).
      Example: `--input="format=htable htable=htable-table-name"`.

    * `kiji`: job input is a Kiji table, each input record is a pair (row entity ID, KijiRowData).
      Example: `--input="format=kiji table=kiji://.env/default/input_table"`.

    * `seq`: job input is a Hadoop sequence file.
      Example: `--input="format=seq file=hdfs://dfsmaster:9000/path/to/sequence-file/"`.

    * `small-text-files`: job input is a set of small text files, each input record is a pair (text file path, text file content).
      Example: `--input="format=small-text-files file=hdfs://dfsmaster:9000/path/to/text-files/"`.

    * `text`: job input is a text file, each input record is a pair (position in the text file, line of text).
      Example: `--input="format=text file=hdfs://dfsmaster:9000/path/to/text-file/"`.

*   `--output=...`: specifies the output of the job.

    The job specification is formatted as: `--output="format= nsplits=N ..."`.
    Kiji recognizes the following job output formats:

    * `avro`: job output is an Avro container file, each output record is a pair (Avro key, NullWritable).

    * `avrokv`: job output is an Avro container file with key/value generic records.

    * `hfile`: job output is an HFile that will be bulk-loaded into a Kiji table.
      Example: `--output="format=hfile nsplits=10 table=kiji://.env/default/target_table file=hdfs://dfsmaster:9000/path/to/hfile/"`.

    * `kiji`: job output is a Kiji table.
      The use of this job output should be limited to development only and should not be used in production as it may incur high load on the target HBase cluster. The exception is producers, whose input and output must be the same kiji table.
      Example: `--output="format=kiji nsplits=10 table=kiji://.env/default/target_table"`.

    * `map`: job output is a Hadoop map file.
      Example: `--output="format=map nsplits=10 file=hdfs://dfsmaster:9000/path/to/map-file/"`.

    * `seq`: job output is a Hadoop sequence file.
      Example: `--output="format=seq nsplits=10 file=hdfs://dfsmaster:9000/path/to/sequence-file/"`.

    * `text`: job output is a text file; each (key, value) record is written to a text file with a separator
      (via the configuration parameter "mapred.textoutputformat.separator", which defaults to TAB) and a new line.
      Example: `--output="format=text nsplits=10 file=hdfs://dfsmaster:9000/path/to/text-file/"`.

# Testing

### Motivation

Kiji comes with a testing framework that makes it very easy to ensure that code you have written works as expected.

### Setup

To use the Kiji testing framework, you must depend on the Kiji testing artifact: `org.kiji.schema:kiji-schema:<kiji-version>:test-jar`.

{% highlight xml %}
<dependency>
  <groupId>org.kiji.schema</groupId>
  <artifactId>kiji-schema</artifactId>
  <version>${kiji-schema.version}</version>
  <type>test-jar</type>
  <scope>test</scope>
</dependency>
{% endhighlight %}

### Classes Overview

Kiji unit tests inherit from the base class `KijiClientTest` which provides an environment suitable for tests of Kiji schema logic and MapReduce logic.
It provides a testing Hadoop configuration accessible through `KijiClientTest.getConf()`.
The KijiClientTest base class keeps track of Kiji instances created for testing.
In particular, it provides a default Kiji instance accessible with `KijiClientTest.getKiji()`.
Other Kiji instances may be created with `KijiClientTest.createKiji()`.

The Kiji testing framework also provides an `InstanceBuilder` helper class to define and populate testing Kiji environments.

### Using the API


{% highlight java %}
public class SomeTests extends KijiClientTest {
  @Test
  public void testSomeFeature() throws Exception {
    // Use the default testing Kiji instance managed by KijiClientTest.
    // No need to release this instance, it is take care of by KijiClientTest.
    final Kiji kiji = getKiji();

    // Create or load a table layout:
    final KijiTableLayout tableLayout = ...;
    final String tableName = tableLayout.getName();

    // Populate the existing Kiji instance 'kiji':
    new InstanceBuilder(kiji)
        // Declare a table
        .withTable(tableName, tableLayout)
            // Declare a row for the entity "Marsellus Wallace":
            .withRow("Marsellus Wallace")
                 .withFamily("info")
                     .withQualifier("first_name").withValue("Marsellus")
                     .withQualifier("last_name").withValue("Wallace")
            // Declare another row for the entity "Vincent Vega":
            .withRow("Vincent Vega")
                 .withFamily("info")
                     .withQualifier("first_name").withValue("Vincent")
                     .withQualifier("last_name").withValue("Vega")
        .build();

    // Open the test table:
    final KijiTable table = kiji.openTable(tableName);
    try {
      // Use the populated table:
      // …
    } finally {
      table.close();
    }
  }
}
{% endhighlight %}

### Example (Unit and integration/single MapReduce job and mutliple in a row)


### Provided Library Classes (Optional)



# <a id="ref.kvstore"> KeyValue Stores </a>

### Motivation

KeyValueStores are used to provide MapReduce programs and other operators processing Kiji datasets with the ability to join datasets. One data set can be specified as a key-value store using the KeyValueStore API. The program can use a KeyValueStoreReader to look up values associated with keys. These keys are often driven by records of a dataset being processed by MapReduce.

This can be used, for example, to provide a `KijiProducer` with the means to apply the results of a trained machine learning model to the main data set. The output of a machine learning model might be expressed as (key, value) pairs stored in files in HDFS, or in a secondary Kiji table. For each user in a users table, you may want to compute a new recommendation for the user by applying the model to the information in the user's row. A value in the user's row may be a key into some arbitrary key-value store representing the model; the returned value is the recommendation.

You may also need to perform "ordinary" map-side joins in a MapReduce program, e.g., for denormalization of data. The smaller dataset can be held in RAM in each map task in the form of a KeyValueStore. For each record in the larger dataset, you can look up the corresponding small-side record, and emit the concatenation of the two to the reducer.

`KeyValueStores` also allow non-MapReduce applications to read key-value pairs from HDFS-backed datasets in a file-format-agnostic fashion. For example, you may run a MapReduce program that emits output as a `SequenceFile` that your frontend application needs to consume. You could use a `SequenceFile.Reader` directly, but if you ever change your MapReduce pipeline to emit to text files or Avro files, you will need to rewrite your client-side logic. Using the KeyValueStoreReader API in your client allows you to decouple the act of using a key-value map from what format you need to use to read the data.

### Classes Overview

The main classes in the KeyValueStore API are `org.kiji.mapreduce.kvstore.KeyValueStore` and `org.kiji.mapreduce.kvstore.KeyValueStoreReader`.

A `KeyValueStore` specifies all the resources needed to surface key-value pairs from some backing store. This may be defined on files, a Kiji table, or some other resource like a different NoSQL database.

Several KeyValueStore implementations are made available in the `org.kiji.mapreduce.kvstore.lib` package that cover common use cases for file- or Kiji-backed datasets. You could write your own `KeyValueStore` implementation that accesses a foreign system (e.g., a Redis database).

A `KeyValueStore` class specifies how to read data into the store. For example, `TextFileKeyValueStore` expects to parse delimited text files, while `SeqFileKeyValueStore` reads SequenceFiles. Both of these stores are configured with an HDFS path to read from. A `KijiTableKeyValueStore` requires different configuration.

The KeyValueStore implementations provided in the library are immutable; they’re all created through builder classes. They each have a method named `builder()` that returns a new instance of the associated builder class.

The `KeyValueStoreReader` API is used to actually look up values by key, from some KeyValueStore. You cannot directly instantiate any concrete implementations of `KeyValueStoreReader` yourself; use a given KeyValueStore's `open()` method to open an associated reader object. The client is agnostic to the backing store; one `KeyValueStoreReader` should act the same as the next, given equivalent backing data.

By default, a KeyValueStoreReader's data is presented to you as a read-only, non-iterable map. Only `get()` requests for an explicit key are supported by default, though some implementations may offer iteration.

An opened KeyValueStoreReader may contain state or connect to external resources; you should call the `close()` method when you are finished using it.

#### Simple Example for Opening a KeyValueStore

{% highlight java %}
    final KeyValueStore<String, String> csvKeyValues = TextFileKeyValueStore.builder()
        .withInputPath(new Path("some-file.txt"))
        .withDistributedCache(false)
        .build();

    final KeyValueStoreReader<String, String> reader = csvKeyValues.open();
    try {
      String theValue = reader.get("some-key");
      System.out.println("Contained a mapping: some-key -> " + theValue);
    } finally {
      reader.close();
    }
{% endhighlight %}

### Using the API

The distinction between KeyValueStore and KeyValueStoreReader is intentional. A given MapReduce program may need to access some set of KeyValueStoreReaders. When the job is configured, you must configure it with any associated KeyValueStores. For example, file-backed stores often use the DistributedCache to efficiently copy files to all map tasks. This must be configured before the job begins.

Your MapReduce task class (KijiMapper, KijiReducer, KijiProducer, KijiGatherer, etc) can implement `KeyValueStoreClient` to help specify to a Kiji job builder (e.g., KijiGatherJobBuilder) that it requires stores. This will require that you implement a method named `getRequiredStores()` that returns a mapping from names to KeyValueStores. These are the default _bindings_ for KeyValueStores.

The use of Java generic types makes constructing the return value from your `getRequiredStores()` method to be cumbersome. Some static factory methods have been added for your convenience in the `RequiredStores` class. For example, to require no stores:

{% highlight java %}
    @Override Map<String, KeyValueStore<?, ?>> getRequiredStores() {
      return RequiredStores.none();
    }
{% endhighlight %}

To require one empty store named `"mystore"`:

{% highlight java %}
    @Override Map<String, KeyValueStore<?, ?>> getRequiredStores() {
      RequiredStores.just("mystore", EmptyKeyValueStore.get());
    }
{% endhighlight %}

The `RequiredStores.with()` method will return a `Map` object augmented with a `with(String, KeyValueStore)` method, so you can call `RequiredStores.with("foo", store1).with("bar", store2);`.

Within the `KijiGatherer.gather()` method, the `KijiContext` object provided as an argument will provide you with access to KeyValueStoreReaders. Since you may need multiple KeyValueStores, you can refer to each by the name you bound it to in `getRequiredStores()`.

Since each call to `gather()` is likely to require the same stores, it would be high-overhead to open and close a KeyValueStoreReader in every call. The KijiContext itself uses a class called `KeyValueStoreReaderFactory`, which is responsible for instantiating KeyValueStores from the `Configuration` associated with the job, and maintaining a pool of lazily-opened KeyValueStoreReader instances organized by name binding. The gatherer itself does not close individual KeyValueStoreReader instances. The KeyValueStoreReaderFactory will close all of them as the task is being torn down.

#### Overriding Default Bindings in Job Builders

While the `getRequiredStores()` method allows you to define bindings between names and implementations, specific invocations of the MapReduce job may require that you override these implementations. For example, you may want to use one HDFS path as input in production, but a different HDFS path in local tests.

The MapReduceJobBuilder subclasses all support a method called `withStore(String name, KeyValueStore store)`. This method allows you to specify a different KeyValueStore implementation for that particular job. You will need to override a store name that the job will expect to read; i.e., if `getRequiredStores()` returned a binding from `"mystore"` to a particular TextFileKeyValueStore, you should call `myJobBuilder.withStore("mystore", myDifferentKeyValueStore);`.

#### Overriding Default Bindings on the Command Line

You can also override KeyValueStore bindings on the command line with the `--kvstores` argument. This argument specifies an XML file that should look like the following:

{% highlight xml %}
<stores>
  <store name="mystore" class="org.kiji.mapreduce.kvstore.lib.TextFileKeyValueStore">
    <configuration>
      <property>
        <name>paths</name>
        <value>/path/to/foo.txt,/path/to/bar.txt</value>
      </property>
      <property>
        <name>delim</name>
        <value>,</value>
      </property>
    </configuration>
  </store>
</stores>
{% endhighlight %}


This example defines a delimited-text-file store, bound to the name `"mystore"`. It reads from `foo.txt` and `bar.txt` and expects a comma between the key and value fields on each line.

You can define multiple name-to-store bindings with different `<store>` blocks with unique names. The properties available within each store's configuration is specified in the Javadoc for each KeyValueStore class.

#### Requiring Runtime Configuration of KeyValueStores

If you know that your KijiProducer requires a store named `"mystore"`, but do not know at compile time where the physical resource that backs it will be, you may want to force your clients to specify this at runtime using one of the above two methods. A mapping of `RequiredStores.just("mystore", UnconfiguredKeyValueStore.get());` will throw an exception when it is serialized to the `Configuration`, which ensures that your job cannot start unless you override the definition with an XML file or a binding in a job builder.


### Example

In this example, suppose we have computed a set of movie sequels, which we will use to naively recommend to users on our web site that if they've watched the first one, they watch the sequel. Suppose we stored the movie sequel dataset in pipe-delimited text files in HDFS, that look something like:

    Star Wars: A New Hope|Star Wars: The Empire Strikes Back
    Raiders of the Lost Ark|Indiana Jones and the Temple of Doom
    ...

We could write a KijiProducer that read from the `info:recently_watched` column and produce a field named `info:recommended_movie` as follows:

{% highlight java %}
    import java.io.IOException;

    import org.apache.hadoop.fs.Path;

    import org.kiji.mapreduce.*;
    import org.kiji.mapreduce.kvstore.*;
    import org.kiji.mapreduce.kvstore.lib.*;
    import org.kiji.schema.*;

    /**
     * Produce a movie recommendation based on the theory that if they liked
     * the first one, they should watch the sequel.
     */
    public class MoveRecProducer extends KijiProducer {
      @Override public Map<String, KeyValueStore<?, ?>> getRequiredStores() {
        // Note that it's ok to specify a path to an HDFS dir, not just one file.
        return RequiredStores.just("sequels", TextFileKeyValueStore.builder()
            .withDelimiter("|")
            .withInputPath(new Path("/path/to/movie-sequels.txt")).build());
      }

      @Override KijiDataRequest getDataRequest() {
        return KijiDataRequest.create("info", "recently_watched");
      }

      @Override String getOutputColumn() {
        return "info:recommended_movie";
      }

    @Override void produce(KijiRowData input, ProducerContext context)
        throws IOException {

        if (!input.containsColumn("info", "recently_watched")) {
          // No basis for recommendation.
          return;
        }

        // Note that we don't call our own getRequiredStores() method. That was used
        // in the configuration phase of the job, not within each task. Its output may have
        // been overridden at run-time by the user. We take the deserialized store from the
        // ProducerContext instead.
        KeyValueStoreReader<String, String> sequelStore = context.getStore("sequels");

        String lastWatched = input.getMostRecentValue("info", "recently_watched").toString();
        String nextMovie = sequelStore.get(lastWatched);
        if (null != nextMovie) {
          // We found a match! Write to info:recommended_movie
          context.write(nextMovie);
        }
      }
    }
{% endhighlight %}


### Provided Library Classes

Several implementations of KeyValueStore are available in the `org.kiji.mapreduce.kvstore.lib` package:

Several file-backed KeyValueStore implementations provide access to stores in different file formats:

* `AvroKVRecordKeyValueStore` - Key-Value pairs specified in Avro records containing two fields, `key` and `value`
* `AvroRecordKeyValueStore` - Avro records in an Avro file, to be indexed by a configurable field of each record.
* `SeqFileKeyValueStore` - Key-Value pairs in SequenceFiles
* `TextFileKeyValueStore` - string key-value pairs in delimited text files

You can also access a specific column of a Kiji table by the row's entityId using the `KijiTableKeyValueStore`.

If you want to declare a name binding on a KeyValueStore whose exact configuration cannot be determined before runtime, use the `UnconfiguredKeyValueStore`. It will throw an IOException in its `storeToConf()` method, ensuring that your `MapReduceJobBuilder` must call `withStore()` to override the definition before launching the job.

The `EmptyKeyValueStore` is a good default choice when you plan to override the configuration at runtime, but find it acceptable to operate without this information.

# <a name="ref.jobhistory">Job History</a>

### Motivation
While Hadoop’s job tracker provides detailed information about jobs that have been run in the cluster, it is not a persistent data store for such information.  After installation of the `job_history` table, Kiji automatically tracks information into the job_history table which can be accessed later.  This information includes an xml dump of the full job configuration, start times, end times, and all job counters.
/

### Setup
The job history tables can be installed into the default Kiji instance with the job-history command:

{% highlight bash %}
kiji job-history --kiji=kiji://.env/default --install
{% endhighlight %}

If you would like to install the job history table into a different instance, pass in the relevant URI into the --kiji parameter.

You can verify that the table was installed properly using the ls command:

{% highlight bash %}
kiji ls --kiji=kiji://.env/default/job_history
{% endhighlight %}

Any new MapReduce jobs that are run within Kiji will now save relevant job related metadata into this table.

### Classes Overview

The `org.kiji.mapreduce.JobHistoryKijiTable` is the main class responsible for providing access to the job_history table.  Currently it provides the ability to record and retrieve job metadata.  This is a framework-audience class and subject to change between minor versions

### Using the API

The `JobHistoryKijiTable` class surfaces the call `getJobDetails` for retrieving the recorded metadata.

Jobs that are created using `KijiMapReduceJob` will automatically record metadata to the `job_history` table if it has been installed.  Any MapReduce jobs using the job builders in Kiji for bulk importers, producers, or gatherers fall under this category and will report data.

### Example

The job_history table is a Kiji table under the hood, and can thus be inspected using the `kiji ls` tool.  The `EntityId` associated with the job_history table is the jobId.  For example, to look at all of the jobIds that have been recorded:

{% highlight bash %}
kiji ls --kiji=kiji://.env/default/job_history --columns=info:jobId
{% endhighlight %}

There is also a job_history tool, which displays the job history data in a more human readable format.  For example, to look up job data for the job ‘job_20130221123621875_0001’

{% highlight bash %}
kiji job-history --kiji=kiji://.env/default --job-id=job_20130221123621875_0001
{% endhighlight %}
